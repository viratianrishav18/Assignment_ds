{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc42e756",
   "metadata": {},
   "source": [
    "### What is Simple Linear Regression?\n",
    "Simple Linear Regression is a statistical method that models the relationship between a dependent variable (Y) and an independent variable (X) using the equation Y = mX + c, where 'm' is the slope and 'c' is the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994fe7d",
   "metadata": {},
   "source": [
    "### What are the key assumptions of Simple Linear Regression?\n",
    "1. Linearity: The relationship between X and Y is linear.\n",
    "2. Independence: Observations are independent.\n",
    "3. Homoscedasticity: Constant variance of residuals.\n",
    "4. Normality: Residuals follow a normal distribution.\n",
    "5. No multicollinearity: Only one independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad080de",
   "metadata": {},
   "source": [
    "### What does the coefficient m represent in the equation Y=mX+c?\n",
    "The coefficient 'm' represents the slope of the regression line, indicating the rate of change in Y for a one-unit increase in X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ab02f",
   "metadata": {},
   "source": [
    "### What does the intercept c represent in the equation Y=mX+c?\n",
    "The intercept 'c' represents the value of Y when X is zero. It is the point where the regression line crosses the Y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa4769",
   "metadata": {},
   "source": [
    "### How do we calculate the slope m in Simple Linear Regression?\n",
    "m = (Σ(X - mean(X)) * (Y - mean(Y))) / Σ(X - mean(X))²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77a795",
   "metadata": {},
   "source": [
    "### What is the purpose of the least squares method in Simple Linear Regression?\n",
    "The least squares method minimizes the sum of squared differences between observed and predicted Y values to find the best-fitting regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa17992",
   "metadata": {},
   "source": [
    "### How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
    "R² measures the proportion of variance in Y explained by X. A value close to 1 indicates a strong relationship, while a value near 0 suggests a weak relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab415f0",
   "metadata": {},
   "source": [
    "### What is Multiple Linear Regression?\n",
    "Multiple Linear Regression models the relationship between a dependent variable and multiple independent variables using Y = b0 + b1X1 + b2X2 + ... + bnXn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d0f88",
   "metadata": {},
   "source": [
    "### What is the main difference between Simple and Multiple Linear Regression?\n",
    "Simple Linear Regression has one independent variable, while Multiple Linear Regression has two or more independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60113ed5",
   "metadata": {},
   "source": [
    "### What are the key assumptions of Multiple Linear Regression?\n",
    "1. Linearity\n",
    "2. Independence\n",
    "3. Homoscedasticity\n",
    "4. Normality\n",
    "5. No multicollinearity (independent variables should not be highly correlated)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991128df",
   "metadata": {},
   "source": [
    "### What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
    "Heteroscedasticity occurs when residual variance is not constant, leading to unreliable standard errors and affecting hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96436075",
   "metadata": {},
   "source": [
    "### How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
    "1. Remove highly correlated variables\n",
    "2. Use Principal Component Analysis (PCA)\n",
    "3. Apply Ridge or Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b01ef5",
   "metadata": {},
   "source": [
    "### What are some common techniques for transforming categorical variables for use in regression models?\n",
    "1. One-Hot Encoding\n",
    "2. Label Encoding\n",
    "3. Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c482eff",
   "metadata": {},
   "source": [
    "### What is the role of interaction terms in Multiple Linear Regression?\n",
    "Interaction terms capture combined effects of independent variables that impact Y beyond their individual contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c5b3d",
   "metadata": {},
   "source": [
    "### How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
    "In Simple Linear Regression, the intercept is the Y value when X is zero. In Multiple Linear Regression, it represents Y when all Xs are zero, which may not always be meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986b64c",
   "metadata": {},
   "source": [
    "### What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
    "The slope represents the rate of change in Y per unit increase in X. It determines the direction and strength of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7bf172",
   "metadata": {},
   "source": [
    "### How does the intercept in a regression model provide context for the relationship between variables?\n",
    "The intercept helps establish a reference point for Y when all predictors are at zero, providing baseline insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501faa5c",
   "metadata": {},
   "source": [
    "### What are the limitations of using R² as a sole measure of model performance?\n",
    "R² does not indicate causation, does not work well with nonlinear relationships, and can be misleading in complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756d839",
   "metadata": {},
   "source": [
    "### How would you interpret a large standard error for a regression coefficient?\n",
    "A large standard error suggests high variability in coefficient estimation, indicating low reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d294e5",
   "metadata": {},
   "source": [
    "### How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
    "Heteroscedasticity appears as a fan-shaped pattern in residual plots. It must be addressed for reliable coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44df99",
   "metadata": {},
   "source": [
    "### What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
    "It indicates that adding independent variables may not be contributing meaningful explanatory power, causing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9cb6e",
   "metadata": {},
   "source": [
    "### Why is it important to scale variables in Multiple Linear Regression?\n",
    "Scaling ensures variables contribute equally, preventing dominance by large-scale features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11367d",
   "metadata": {},
   "source": [
    "### What is polynomial regression?\n",
    "Polynomial Regression is an extension of linear regression where the relationship between independent and dependent variables is modeled as an nth-degree polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbbd997",
   "metadata": {},
   "source": [
    "### How does polynomial regression differ from linear regression?\n",
    "Polynomial Regression captures nonlinear relationships by including higher-degree terms of the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535379d",
   "metadata": {},
   "source": [
    "### When is polynomial regression used?\n",
    "It is used when data shows a curvilinear trend that a linear model cannot capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119921e",
   "metadata": {},
   "source": [
    "### What is the general equation for polynomial regression?\n",
    "Y = b0 + b1X + b2X² + ... + bnXⁿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8f198",
   "metadata": {},
   "source": [
    "### Can polynomial regression be applied to multiple variables?\n",
    "Yes, it can be extended to multiple independent variables, leading to Polynomial Multiple Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8defe3e3",
   "metadata": {},
   "source": [
    "### What are the limitations of polynomial regression?\n",
    "1. Overfitting with high-degree polynomials\n",
    "2. Computationally expensive\n",
    "3. Sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e8b4c",
   "metadata": {},
   "source": [
    "### What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
    "1. R² Score\n",
    "2. Cross-validation\n",
    "3. Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ac2b0",
   "metadata": {},
   "source": [
    "### Why is visualization important in polynomial regression?\n",
    "Visualization helps identify the polynomial degree required to capture patterns effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Practical Example: Polynomial Regression in Python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generating sample data\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([2.1, 2.9, 3.8, 5.2, 6.8, 8.7, 11.1, 13.9, 17.5, 21.2])\n",
    "\n",
    "# Applying polynomial transformation\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Fitting the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Predicting values\n",
    "y_pred = model.predict(X_poly)\n",
    "\n",
    "# Plotting results\n",
    "plt.scatter(X, y, color='blue', label=\"Original Data\")\n",
    "plt.plot(X, y_pred, color='red', label=\"Polynomial Regression\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Polynomial Regression Example\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}