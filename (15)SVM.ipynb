{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "HUlQCqCE_aKw",
        "outputId": "5057a960-1740-4bf6-a5a3-457c67d1329e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Number of informative, redundant and repeated features must sum to less than the number of total features",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0ee70131ab6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mC_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_samples_generator.py\u001b[0m in \u001b[0;36mmake_classification\u001b[0;34m(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m# Count features, clusters and samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_informative\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_redundant\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_repeated\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;34m\"Number of informative, redundant and repeated \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;34m\"features must sum to less than the number of total\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of informative, redundant and repeated features must sum to less than the number of total features"
          ]
        }
      ],
      "source": [
        "# 21. Train an SVM Classifier with different C values and compare decision boundaries\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)\n",
        "C_values = [0.1, 1, 10]\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, C in enumerate(C_values, 1):\n",
        "    svm = SVC(C=C, kernel='linear').fit(X, y)\n",
        "    plt.subplot(1, 3, i)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
        "    plt.title(f'C = {C}')\n",
        "plt.show()\n",
        "\n",
        "# 22. Train a Bernoulli Naïve Bayes classifier for binary classification\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "nb = BernoulliNB().fit(X_train, y_train)\n",
        "print(\"22. BernoulliNB Accuracy:\", accuracy_score(y_test, nb.predict(X_test)))\n",
        "\n",
        "# 23. Apply feature scaling before training an SVM model and compare results\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "svm_scaled = SVC(kernel='linear').fit(X_train_scaled, y_train)\n",
        "print(\"23. SVM Accuracy with Scaling:\", accuracy_score(y_test, svm_scaled.predict(X_test_scaled)))\n",
        "\n",
        "# 24. Train a Gaussian Naïve Bayes model and compare predictions before and after Laplace Smoothing\n",
        "nb_no_smoothing = GaussianNB(var_smoothing=1e-9)\n",
        "nb_with_smoothing = GaussianNB(var_smoothing=1e-5)\n",
        "nb_no_smoothing.fit(X_train, y_train)\n",
        "nb_with_smoothing.fit(X_train, y_train)\n",
        "print(\"24. Accuracy without smoothing:\", accuracy_score(y_test, nb_no_smoothing.predict(X_test)))\n",
        "print(\"24. Accuracy with smoothing:\", accuracy_score(y_test, nb_with_smoothing.predict(X_test)))\n",
        "\n",
        "# 25. Train an SVM Classifier and use GridSearchCV to tune hyperparameters\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']}\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"25. Best parameters for SVM:\", grid_search.best_params_)\n",
        "\n",
        "# 26. Train an SVM Classifier on an imbalanced dataset using class weighting\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "svm_balanced = SVC(kernel='linear', class_weight=weights_dict).fit(X_train, y_train)\n",
        "print(\"26. SVM Accuracy on Imbalanced Data:\", accuracy_score(y_test, svm_balanced.predict(X_test)))\n",
        "\n",
        "# 27. Train a Naïve Bayes classifier for spam detection\n",
        "# (Requires an email dataset)\n",
        "\n",
        "# 28. Train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare accuracy\n",
        "nb = GaussianNB().fit(X_train, y_train)\n",
        "svm = SVC(kernel='linear').fit(X_train, y_train)\n",
        "print(\"28. SVM Accuracy:\", accuracy_score(y_test, svm.predict(X_test)))\n",
        "print(\"28. Naïve Bayes Accuracy:\", accuracy_score(y_test, nb.predict(X_test)))\n",
        "\n",
        "# 29. Perform feature selection before training a Naïve Bayes classifier\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "selector = SelectKBest(chi2, k=2)\n",
        "X_new = selector.fit_transform(X_train, y_train)\n",
        "nb_selected = GaussianNB().fit(X_new, y_train)\n",
        "print(\"29. Naïve Bayes Accuracy after Feature Selection:\", accuracy_score(y_test, nb_selected.predict(selector.transform(X_test))))\n",
        "\n",
        "# 30. Train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies\n",
        "svm_ovr = SVC(decision_function_shape='ovr').fit(X_train, y_train)\n",
        "svm_ovo = SVC(decision_function_shape='ovo').fit(X_train, y_train)\n",
        "print(\"30. OvR Accuracy:\", accuracy_score(y_test, svm_ovr.predict(X_test)))\n",
        "print(\"30. OvO Accuracy:\", accuracy_score(y_test, svm_ovo.predict(X_test)))\n",
        "\n",
        "# 31. Train an SVM Classifier using Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "avg_accuracy = cross_val_score(SVC(kernel='linear'), X_train, y_train, cv=skf).mean()\n",
        "print(\"31. Stratified K-Fold Accuracy:\", avg_accuracy)\n",
        "\n",
        "# 32. Train a Naïve Bayes classifier using different prior probabilities\n",
        "nb_prior = GaussianNB(priors=[0.7, 0.3])\n",
        "nb_prior.fit(X_train, y_train)\n",
        "print(\"32. Naïve Bayes Accuracy with Prior Probabilities:\", accuracy_score(y_test, nb_prior.predict(X_test)))\n",
        "\n",
        "# 33. Perform Recursive Feature Elimination (RFE) before training an SVM Classifier\n",
        "from sklearn.feature_selection import RFE\n",
        "rfe = RFE(SVC(kernel='linear'), n_features_to_select=2)\n",
        "X_rfe = rfe.fit_transform(X_train, y_train)\n",
        "svm_rfe = SVC(kernel='linear').fit(X_rfe, y_train)\n",
        "print(\"33. SVM Accuracy after RFE:\", accuracy_score(y_test, svm_rfe.predict(rfe.transform(X_test))))\n",
        "\n",
        "# 34. Train an SVM Classifier and evaluate using Precision, Recall, and F1-Score\n",
        "y_pred = svm.predict(X_test)\n",
        "print(\"34. Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"34. Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"34. F1-Score:\", f1_score(y_test, y_pred, average='macro'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 40. Train an SVM Classifier and evaluate using Precision, Recall, and F1-Score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = svm.predict(X_test)\n",
        "print(\"40. Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"40. Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"40. F1-Score:\", f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "# 41. Train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n",
        "y_pred_proba = nb.predict_proba(X_test)\n",
        "from sklearn.metrics import log_loss\n",
        "print(\"41. Log Loss:\", log_loss(y_test, y_pred_proba))\n",
        "\n",
        "# 42. Train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"42. Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# 43. Train an SVM Regressor (SVR) and evaluate using Mean Absolute Error (MAE)\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "y_pred_svr = svr.predict(X_test)\n",
        "print(\"43. SVR MAE:\", mean_absolute_error(y_test, y_pred_svr))\n",
        "\n",
        "# 44. Train a Naïve Bayes classifier and evaluate performance using ROC-AUC score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_pred_prob = nb.predict_proba(X_test)[:, 1]\n",
        "print(\"44. ROC-AUC Score:\", roc_auc_score(y_test, y_pred_prob))\n",
        "\n",
        "# 45. Train an SVM Classifier and visualize the Precision-Recall Curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"45. Precision-Recall Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Oj2nU5eQC5iq",
        "outputId": "fb3af35b-a8b5-4413-ffff-0deba4a3d1b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'svm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b9235c443536>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"40. Precision:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"40. Recall:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'svm' is not defined"
          ]
        }
      ]
    }
  ]
}